## Openclip-flashattn

Test model (on RTX3080): `ViT-L-14::laion2b-s32b-b82k` (possibly not correct)

Times = **100**

shape   | baseline(s)        | flash_attn(s)  | speed up (x) | mean diff
--------|---------------------|-----------------|--------------|-----------
(1, 77) | 0.64022             | 0.55076         | 1.1624       | 0.000733
(2, 77) | 0.67274             | 0.59914         | 1.1228       | 0.000628
(4, 77) | 0.67366             | 0.60010         | 1.1225       | 0.000573
(8, 77) | 0.71364             | 0.63930         | 1.1162       | 0.000528
(16, 77)| 0.76330             | 0.73598         | 1.0469       | 0.000546
(1, 3, 224, 224)|1.25425      | 1.10564         | 1.1344       | 0.001713
(2, 3, 224, 224)|1.32202      | 1.22698         | 1.0775       | 0.001739
(4, 3, 224, 224)|2.36880      | 2.11119         | 1.0926       | 0.001227
(8, 3, 224, 224)|3.88597      | 3.76666         | 1.0316       | 0.001441
(16, 3, 224, 224)|7.84954      |7.20817         | 1.0889       | 0.001533


Only count the time cost of attention computation (not the entire layer):
shape   | baseline(us)        | flash_attn(us)  | speed up (x)
--------|---------------------|-----------------|-------------
(1, 77) | 678.196             | 379.091         | 1,7890
(2, 77) | 629.695             | 383.335         | 1.6436
(4, 77) | 634.543             | 389.085         | 1.6309
(8, 77) | 624.595             | 375.954         | 1.6614
(16, 77)| 663.095             | 353.379         | 1.8764