## Openclip-flashattn

Test model (on RTX3080): `ViT-L-14::laion2b-s32b-b82k` (possibly not correct)

Times = **1000**

shape   | baseline(s)        | flash_attn(s)  | speed up (x) | mean diff
--------|---------------------|-----------------|--------------|-----------
(1, 77) | 0.62664             | 0.55889         | 1.1212       | 0.000733
(2, 77) | 0.67169             | 0.61476         | 1.0926       | 0.000628
(4, 77) | 0.74454             | 0.67252         | 1.0944       | 0.000573
(8, 77) | 0.66874             | 0.62384         | 1.0719       | 0.000528
(16, 77)| 0.79788             | 0.76210         | 1.0469       | 0.000546
(1, 3, 224, 224)|1.25425      | 1.10564         | 1.1344       | 0.001713
(2, 3, 224, 224)|1.87309      | 1.27422         | 1.4699       | 0.001739
(4, 3, 224, 224)|2.30689      | 2.11119         | 1.0926       | 0.001227
(8, 3, 224, 224)|3.88597      | 3.76666         | 1.0316       | 0.001441
(16, 3, 224, 224)|7.84954      |7.20817         | 1.0889       | 0.001533


Only count the time cost of attention computation (not the entire layer):
shape   | baseline(us)        | flash_attn(us)  | speed up (x)
--------|---------------------|-----------------|-------------
(1, 77) | 678.196             | 379.091         | 1,7890
(2, 77) | 629.695             | 383.335         | 1.6436
(4, 77) | 634.543             | 389.085         | 1.6309
(8, 77) | 624.595             | 375.954         | 1.6614
(16, 77)| 663.095             | 353.379         | 1.8764